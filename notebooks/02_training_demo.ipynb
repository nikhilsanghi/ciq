{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QLoRA Training Demo\n",
    "\n",
    "This notebook demonstrates QLoRA (Quantized Low-Rank Adaptation) fine-tuning for our e-commerce LLM.\n",
    "\n",
    "## What is QLoRA?\n",
    "\n",
    "QLoRA enables fine-tuning of large language models on consumer hardware by combining:\n",
    "1. **4-bit Quantization**: Reduces model memory footprint by 4x\n",
    "2. **LoRA Adapters**: Trains small adapter layers instead of full model weights\n",
    "3. **Paged Optimizers**: Handles memory spikes during training\n",
    "\n",
    "### Memory Savings:\n",
    "- Full Mistral-7B: ~28GB (FP16) or ~14GB (FP16 with gradient checkpointing)\n",
    "- QLoRA Mistral-7B: ~5-6GB (4-bit + LoRA adapters)\n",
    "\n",
    "### Training Parameters:\n",
    "- Base model: Mistral-7B-Instruct-v0.3\n",
    "- Quantization: 4-bit NF4 with double quantization\n",
    "- LoRA rank: 32, alpha: 64\n",
    "- Target: all linear layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if needed\n",
    "# !pip install torch transformers peft trl bitsandbytes datasets accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from trl import SFTTrainer\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding QLoRA Components\n",
    "\n",
    "### 1.1 4-bit Quantization with NF4\n",
    "\n",
    "NF4 (NormalFloat4) is a 4-bit quantization format optimized for normally distributed weights:\n",
    "\n",
    "```\n",
    "Standard FP16:  [sign(1)] [exponent(5)] [mantissa(10)] = 16 bits\n",
    "NF4:            [quantized_value(4)]                   = 4 bits\n",
    "\n",
    "Memory reduction: 16/4 = 4x smaller\n",
    "```\n",
    "\n",
    "### 1.2 Double Quantization\n",
    "\n",
    "Quantizes the quantization constants themselves for additional memory savings:\n",
    "- First quantization: FP16 -> 4-bit (with FP16 scale factors)\n",
    "- Second quantization: FP16 scale factors -> 8-bit\n",
    "\n",
    "### 1.3 LoRA (Low-Rank Adaptation)\n",
    "\n",
    "Instead of updating all model weights, LoRA adds small trainable matrices:\n",
    "\n",
    "```\n",
    "Original:  W_new = W_original + delta_W\n",
    "LoRA:      W_new = W_original + (A @ B)  where A: [d, r], B: [r, d]\n",
    "\n",
    "With rank r=32 and dimension d=4096:\n",
    "- Full fine-tuning: 4096 * 4096 = 16.7M parameters\n",
    "- LoRA: 4096 * 32 + 32 * 4096 = 262K parameters (64x fewer!)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual representation of LoRA\n",
    "print(\"\"\"\n",
    "LoRA Architecture:\n",
    "==================\n",
    "\n",
    "                    Input (x)\n",
    "                       |\n",
    "           +-----------+-----------+\n",
    "           |                       |\n",
    "           v                       v\n",
    "    +-------------+         +-------------+\n",
    "    |   W_frozen  |         |    A (down) |  <- Trainable (d x r)\n",
    "    | (d x d)     |         +-------------+\n",
    "    | 4-bit quant |                |\n",
    "    +-------------+                v\n",
    "           |              +-------------+\n",
    "           |              |    B (up)   |  <- Trainable (r x d)\n",
    "           |              +-------------+\n",
    "           |                       |\n",
    "           +-----------+-----------+\n",
    "                       | (add)\n",
    "                       v\n",
    "                    Output\n",
    "\n",
    "Where:\n",
    "  - d = hidden dimension (4096 for 7B models)\n",
    "  - r = LoRA rank (typically 8-64)\n",
    "  - W_frozen = original weights (quantized to 4-bit)\n",
    "  - A, B = LoRA adapter matrices (trained in FP16/BF16)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Model with 4-bit Quantization\n",
    "\n",
    "We'll load Mistral-7B with 4-bit quantization and compare VRAM usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "OUTPUT_DIR = \"./checkpoints/ecommerce-qlora\"\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gpu_memory():\n",
    "    \"\"\"Get current GPU memory usage in GB.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1e9\n",
    "        reserved = torch.cuda.memory_reserved() / 1e9\n",
    "        return allocated, reserved\n",
    "    return 0, 0\n",
    "\n",
    "def print_gpu_memory(label=\"\"):\n",
    "    \"\"\"Print current GPU memory usage.\"\"\"\n",
    "    allocated, reserved = get_gpu_memory()\n",
    "    print(f\"{label}\")\n",
    "    print(f\"  GPU Memory Allocated: {allocated:.2f} GB\")\n",
    "    print(f\"  GPU Memory Reserved:  {reserved:.2f} GB\")\n",
    "\n",
    "# Initial memory\n",
    "print_gpu_memory(\"Initial GPU Memory:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define 4-bit quantization configuration\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,                    # Enable 4-bit quantization\n",
    "    bnb_4bit_quant_type=\"nf4\",           # Use NormalFloat4 format\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16, # Compute in BF16 for stability\n",
    "    bnb_4bit_use_double_quant=True,       # Enable double quantization\n",
    ")\n",
    "\n",
    "print(\"BitsAndBytes Configuration:\")\n",
    "print(f\"  load_in_4bit: {bnb_config.load_in_4bit}\")\n",
    "print(f\"  bnb_4bit_quant_type: {bnb_config.bnb_4bit_quant_type}\")\n",
    "print(f\"  bnb_4bit_compute_dtype: {bnb_config.bnb_4bit_compute_dtype}\")\n",
    "print(f\"  bnb_4bit_use_double_quant: {bnb_config.bnb_4bit_use_double_quant}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "\n",
    "# Set padding token (required for training)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "tokenizer.padding_side = \"right\"  # Required for causal LM training\n",
    "\n",
    "print(f\"Vocabulary size: {tokenizer.vocab_size:,}\")\n",
    "print(f\"Pad token: {tokenizer.pad_token}\")\n",
    "print(f\"EOS token: {tokenizer.eos_token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model with 4-bit quantization\n",
    "print(\"\\nLoading model with 4-bit quantization...\")\n",
    "print(\"(This may take a few minutes on first download)\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "print(\"\\nModel loaded successfully!\")\n",
    "print_gpu_memory(\"GPU Memory After Loading 4-bit Model:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show model architecture summary\n",
    "print(\"\\nModel Architecture Summary:\")\n",
    "print(f\"  Model type: {model.config.model_type}\")\n",
    "print(f\"  Hidden size: {model.config.hidden_size}\")\n",
    "print(f\"  Num layers: {model.config.num_hidden_layers}\")\n",
    "print(f\"  Num attention heads: {model.config.num_attention_heads}\")\n",
    "print(f\"  Vocab size: {model.config.vocab_size:,}\")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nParameter Count:\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"  Trainable %: {100 * trainable_params / total_params:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configure LoRA Adapters\n",
    "\n",
    "We'll add LoRA adapters to all linear layers for comprehensive fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare model for k-bit training (enables gradient checkpointing)\n",
    "model = prepare_model_for_kbit_training(\n",
    "    model,\n",
    "    use_gradient_checkpointing=True,\n",
    ")\n",
    "\n",
    "print(\"Model prepared for k-bit training with gradient checkpointing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all linear layer names for LoRA targeting\n",
    "def find_all_linear_names(model):\n",
    "    \"\"\"Find all linear layer names in the model.\"\"\"\n",
    "    cls = torch.nn.Linear\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[-1])\n",
    "    \n",
    "    # Remove output layer if present\n",
    "    if 'lm_head' in lora_module_names:\n",
    "        lora_module_names.remove('lm_head')\n",
    "    \n",
    "    return list(lora_module_names)\n",
    "\n",
    "target_modules = find_all_linear_names(model)\n",
    "print(f\"Target modules for LoRA: {target_modules}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=32,                          # LoRA rank (higher = more capacity, more memory)\n",
    "    lora_alpha=64,                 # LoRA scaling factor (typically 2*r)\n",
    "    target_modules=target_modules,  # Apply to all linear layers\n",
    "    lora_dropout=0.05,             # Dropout for regularization\n",
    "    bias=\"none\",                   # Don't train biases\n",
    "    task_type=\"CAUSAL_LM\",         # Causal language modeling\n",
    ")\n",
    "\n",
    "print(\"\\nLoRA Configuration:\")\n",
    "print(f\"  Rank (r): {lora_config.r}\")\n",
    "print(f\"  Alpha: {lora_config.lora_alpha}\")\n",
    "print(f\"  Scaling factor: {lora_config.lora_alpha / lora_config.r}\")\n",
    "print(f\"  Dropout: {lora_config.lora_dropout}\")\n",
    "print(f\"  Target modules: {lora_config.target_modules}\")\n",
    "print(f\"  Task type: {lora_config.task_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply LoRA adapters to model\n",
    "print(\"\\nApplying LoRA adapters...\")\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "print_gpu_memory(\"\\nGPU Memory After Adding LoRA Adapters:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual comparison of parameter efficiency\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "frozen_params = total_params - trainable_params\n",
    "\n",
    "print(\"\"\"\n",
    "Parameter Efficiency Comparison:\n",
    "================================\n",
    "\n",
    "Full Fine-tuning:    [##############################] ~7B params (100%)\n",
    "                      All parameters updated every step\n",
    "                      VRAM: ~28GB (FP16) or ~56GB (FP32)\n",
    "\n",
    "QLoRA Fine-tuning:   [#]                              ~20M params (~0.3%)\n",
    "                      Only LoRA adapters updated\n",
    "                      VRAM: ~5-6GB (4-bit base + FP16 adapters)\n",
    "\"\"\")\n",
    "\n",
    "print(f\"\\nActual numbers for this model:\")\n",
    "print(f\"  Total parameters:     {total_params:>15,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:>15,}\")\n",
    "print(f\"  Frozen parameters:    {frozen_params:>15,}\")\n",
    "print(f\"  Training efficiency:  {100 * trainable_params / total_params:.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prepare Training Data\n",
    "\n",
    "We'll use a small subset of ECInstruct for this demo (100 examples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ECInstruct dataset\n",
    "print(\"Loading ECInstruct dataset...\")\n",
    "ecinstruct = load_dataset(\"NingLab/ECInstruct\", split=\"train\")\n",
    "print(f\"Total examples: {len(ecinstruct):,}\")\n",
    "\n",
    "# Take a small sample for demo\n",
    "DEMO_SIZE = 100\n",
    "demo_dataset = ecinstruct.shuffle(seed=42).select(range(DEMO_SIZE))\n",
    "print(f\"Demo dataset size: {len(demo_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prompt formatting function\n",
    "def format_instruction_prompt(example):\n",
    "    \"\"\"Format an example into the training prompt format.\"\"\"\n",
    "    instruction = example.get('instruction', '')\n",
    "    input_text = example.get('input', '')\n",
    "    output = example.get('output', '')\n",
    "    \n",
    "    # Determine task type and add prefix\n",
    "    instruction_lower = instruction.lower()\n",
    "    if any(kw in instruction_lower for kw in ['classify', 'category', 'categorize']):\n",
    "        task_prefix = \"[CLASSIFY] \"\n",
    "    elif any(kw in instruction_lower for kw in ['extract', 'attribute', 'specification']):\n",
    "        task_prefix = \"[EXTRACT] \"\n",
    "    elif any(kw in instruction_lower for kw in ['question', 'answer', 'what', 'how']):\n",
    "        task_prefix = \"[QA] \"\n",
    "    else:\n",
    "        task_prefix = \"\"\n",
    "    \n",
    "    # Format prompt using Mistral's chat template style\n",
    "    if input_text and input_text.strip():\n",
    "        prompt = f\"\"\"<s>[INST] {task_prefix}{instruction}\n",
    "\n",
    "Input: {input_text} [/INST] {output}</s>\"\"\"\n",
    "    else:\n",
    "        prompt = f\"\"\"<s>[INST] {task_prefix}{instruction} [/INST] {output}</s>\"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "# Test formatting\n",
    "sample = demo_dataset[0]\n",
    "formatted = format_instruction_prompt(sample)\n",
    "print(\"Sample formatted prompt:\")\n",
    "print(\"=\" * 60)\n",
    "print(formatted[:1000])\n",
    "print(\"...\" if len(formatted) > 1000 else \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format dataset\n",
    "def formatting_prompts_func(examples):\n",
    "    \"\"\"Format a batch of examples.\"\"\"\n",
    "    texts = []\n",
    "    for i in range(len(examples['instruction'])):\n",
    "        example = {\n",
    "            'instruction': examples['instruction'][i],\n",
    "            'input': examples['input'][i] if 'input' in examples else '',\n",
    "            'output': examples['output'][i],\n",
    "        }\n",
    "        texts.append(format_instruction_prompt(example))\n",
    "    return {'text': texts}\n",
    "\n",
    "# Apply formatting\n",
    "demo_dataset = demo_dataset.map(\n",
    "    formatting_prompts_func,\n",
    "    batched=True,\n",
    "    remove_columns=demo_dataset.column_names,\n",
    ")\n",
    "\n",
    "print(f\"Formatted dataset columns: {demo_dataset.column_names}\")\n",
    "print(f\"Sample text length: {len(demo_dataset[0]['text'])} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run Mini Training\n",
    "\n",
    "We'll train for a few steps to demonstrate the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=10,\n",
    "    max_steps=25,  # Limit steps for demo\n",
    "    learning_rate=2e-4,\n",
    "    fp16=False,\n",
    "    bf16=True,  # Use BF16 for training stability\n",
    "    logging_steps=5,\n",
    "    save_steps=25,\n",
    "    save_total_limit=2,\n",
    "    optim=\"paged_adamw_8bit\",  # Memory-efficient optimizer\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    report_to=\"none\",  # Disable wandb for demo\n",
    "    gradient_checkpointing=True,\n",
    "    max_grad_norm=0.3,\n",
    ")\n",
    "\n",
    "print(\"Training Arguments:\")\n",
    "print(f\"  Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  Gradient accumulation: {training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  Max steps: {training_args.max_steps}\")\n",
    "print(f\"  Optimizer: {training_args.optim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SFT trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=demo_dataset,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=2048,\n",
    "    dataset_text_field=\"text\",\n",
    "    packing=False,  # Disable packing for simplicity\n",
    ")\n",
    "\n",
    "print(\"SFT Trainer created successfully!\")\n",
    "print_gpu_memory(\"GPU Memory Before Training:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test generation before training\n",
    "def generate_response(prompt, max_new_tokens=256):\n",
    "    \"\"\"Generate a response for a given prompt.\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.1,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# Test prompt\n",
    "test_prompt = \"\"\"<s>[INST] [CLASSIFY] Classify this product into Google Product Taxonomy categories:\n",
    "\n",
    "Input: Apple MacBook Pro 14-inch with M3 Pro chip, 18GB RAM, 512GB SSD, Space Gray [/INST]\"\"\"\n",
    "\n",
    "print(\"Output BEFORE training:\")\n",
    "print(\"=\" * 60)\n",
    "before_response = generate_response(test_prompt)\n",
    "print(before_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training\n",
    "print(\"\\nStarting training...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "start_time = datetime.now()\n",
    "train_result = trainer.train()\n",
    "end_time = datetime.now()\n",
    "\n",
    "print(\"\\nTraining completed!\")\n",
    "print(f\"Duration: {end_time - start_time}\")\n",
    "print(f\"Final loss: {train_result.training_loss:.4f}\")\n",
    "\n",
    "print_gpu_memory(\"\\nGPU Memory After Training:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test generation after training\n",
    "print(\"\\nOutput AFTER training:\")\n",
    "print(\"=\" * 60)\n",
    "after_response = generate_response(test_prompt)\n",
    "print(after_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare before and after\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"COMPARISON: Before vs After Training\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n[BEFORE TRAINING]\")\n",
    "print(\"-\" * 40)\n",
    "print(before_response[len(test_prompt):] if test_prompt in before_response else before_response)\n",
    "\n",
    "print(\"\\n[AFTER TRAINING]\")\n",
    "print(\"-\" * 40)\n",
    "print(after_response[len(test_prompt):] if test_prompt in after_response else after_response)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Note: With only 25 training steps, improvements may be subtle.\")\n",
    "print(\"Full training (3 epochs) produces more significant improvements.\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save and Load Checkpoints\n",
    "\n",
    "One advantage of LoRA is that we only need to save the small adapter weights, not the full model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save LoRA adapters\n",
    "adapter_path = os.path.join(OUTPUT_DIR, \"final_adapter\")\n",
    "model.save_pretrained(adapter_path)\n",
    "tokenizer.save_pretrained(adapter_path)\n",
    "\n",
    "print(f\"\\nAdapter saved to: {adapter_path}\")\n",
    "\n",
    "# Check adapter size\n",
    "import subprocess\n",
    "result = subprocess.run(['du', '-sh', adapter_path], capture_output=True, text=True)\n",
    "print(f\"Adapter size: {result.stdout.strip().split()[0]}\")\n",
    "\n",
    "# List saved files\n",
    "print(\"\\nSaved files:\")\n",
    "for f in os.listdir(adapter_path):\n",
    "    file_path = os.path.join(adapter_path, f)\n",
    "    size = os.path.getsize(file_path) / 1e6  # Size in MB\n",
    "    print(f\"  {f}: {size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate loading the adapter\n",
    "print(\"\\nDemonstrating adapter loading...\")\n",
    "print(\"(In practice, you would load into a fresh model)\")\n",
    "\n",
    "# Save adapter config for reference\n",
    "config_info = {\n",
    "    \"base_model\": MODEL_NAME,\n",
    "    \"lora_rank\": lora_config.r,\n",
    "    \"lora_alpha\": lora_config.lora_alpha,\n",
    "    \"target_modules\": lora_config.target_modules,\n",
    "    \"training_steps\": training_args.max_steps,\n",
    "    \"final_loss\": train_result.training_loss,\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "}\n",
    "\n",
    "with open(os.path.join(adapter_path, \"training_info.json\"), \"w\") as f:\n",
    "    json.dump(config_info, f, indent=2)\n",
    "\n",
    "print(\"\\nTraining info saved:\")\n",
    "print(json.dumps(config_info, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to load adapter in a new session (for reference)\n",
    "print(\"\"\"\n",
    "To load the trained adapter in a new session:\n",
    "==============================================\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "# Load base model with quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# Load LoRA adapter\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    \"./checkpoints/ecommerce-qlora/final_adapter\",\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"./checkpoints/ecommerce-qlora/final_adapter\"\n",
    ")\n",
    "\n",
    "# Model is ready for inference!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary and Next Steps\n",
    "\n",
    "### What We Learned:\n",
    "\n",
    "1. **QLoRA Architecture**:\n",
    "   - 4-bit NF4 quantization reduces memory by 4x\n",
    "   - LoRA adapters train only ~0.3% of parameters\n",
    "   - Combined savings enable 7B model training on ~6GB VRAM\n",
    "\n",
    "2. **Configuration**:\n",
    "   - LoRA rank=32, alpha=64 for good quality-efficiency tradeoff\n",
    "   - Target all linear layers for comprehensive adaptation\n",
    "   - Use paged_adamw_8bit optimizer for memory efficiency\n",
    "\n",
    "3. **Adapter Management**:\n",
    "   - Adapters are small (~100-200MB) vs full model (~14GB)\n",
    "   - Can maintain multiple task-specific adapters\n",
    "   - Easy to share and version control\n",
    "\n",
    "### Production Training Recommendations:\n",
    "\n",
    "```python\n",
    "# Full training configuration\n",
    "training_args = TrainingArguments(\n",
    "    num_train_epochs=3,              # 3 epochs to avoid overfitting\n",
    "    per_device_train_batch_size=4,   # Adjust based on VRAM\n",
    "    gradient_accumulation_steps=4,   # Effective batch size = 16\n",
    "    learning_rate=2e-4,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    max_grad_norm=0.3,\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    ")\n",
    "```\n",
    "\n",
    "### Next Steps:\n",
    "1. Run full training on complete ECInstruct + 10% Alpaca mixture\n",
    "2. Evaluate on held-out test set\n",
    "3. Deploy with vLLM for production inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up GPU memory\n",
    "print(\"Cleaning up...\")\n",
    "\n",
    "del model\n",
    "del trainer\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print_gpu_memory(\"Final GPU Memory:\")\n",
    "print(\"\\nNotebook complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
